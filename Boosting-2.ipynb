{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87aacc3c-0fb1-46ba-b50a-3aaf5c7b295b",
   "metadata": {},
   "source": [
    "# Q1. What is Gradient Boosting Regression?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7904eec-1c31-4673-91be-ada5cb09a4e2",
   "metadata": {},
   "source": [
    "Gradient Boosting Regression is a machine learning technique used for regression tasks. It is an ensemble learning method that combines the predictions of multiple weak learners (typically decision trees) to create a strong learner. The key idea behind Gradient Boosting Regression is to sequentially train a series of regression trees, each one trained to correct the errors of the previous trees.\n",
    "\n",
    "Here's a high-level overview of how Gradient Boosting Regression works:\n",
    "\n",
    "1. **Initialize the Model:** The initial model is set as a simple regression model, usually the mean of the target variable.\n",
    "\n",
    "2. **Fit a Tree:** A regression tree is fit to the residuals (the differences between the actual and predicted values) of the current model. This tree is trained to predict the residuals, which are the errors made by the current model.\n",
    "\n",
    "3. **Update the Model:** The predictions of the new tree are added to the predictions of the current model, updating the model's predictions.\n",
    "\n",
    "4. **Repeat:** Steps 2 and 3 are repeated for a specified number of iterations (or until a stopping criterion is met). Each new tree is trained to predict the residuals of the current model, with the model's predictions being updated after each tree is fit.\n",
    "\n",
    "5. **Final Prediction:** The final prediction is the sum of the predictions of all the trees in the ensemble.\n",
    "\n",
    "Gradient Boosting Regression is known for its ability to handle complex relationships in the data and its resistance to overfitting. However, it can be computationally expensive and sensitive to hyperparameters, so care must be taken when tuning the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e424a65a-0ae3-4d23-b206-75b1a1360828",
   "metadata": {},
   "source": [
    "# Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a simple regression problem as an example and train the model on a small dataset. Evaluate the model's performance using metrics such as mean squared error and R-squared.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7751c3cc-008e-40b8-989f-e652d9e47b88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class GradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.trees = []\n",
    "        self.tree_weights = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Initialize the model predictions as the mean of the target variable\n",
    "        self.base_prediction = np.mean(y)\n",
    "        y_pred = np.full_like(y, self.base_prediction)\n",
    "        \n",
    "        for _ in range(self.n_estimators):\n",
    "            # Calculate the residuals\n",
    "            residuals = y - y_pred\n",
    "\n",
    "            # Fit a regression tree to the residuals\n",
    "            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "            tree.fit(X, residuals)\n",
    "\n",
    "            # Calculate the tree weight using the learning rate\n",
    "            tree_weight = self.learning_rate\n",
    "            self.tree_weights.append(tree_weight)\n",
    "\n",
    "            # Update the model predictions\n",
    "            y_pred += tree_weight * tree.predict(X)\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Initialize predictions as the base prediction\n",
    "        y_pred = np.full(X.shape[0], self.base_prediction)\n",
    "\n",
    "        # Add the predictions of each tree weighted by the tree weight\n",
    "        for tree, tree_weight in zip(self.trees, self.tree_weights):\n",
    "            y_pred += tree_weight * tree.predict(X)\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56a15db4-9487-4e59-902f-249603efdd68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example usage\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbf5e124-0c15-4b14-abfd-2132526f1c05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate a small regression dataset\n",
    "X, y = make_regression(n_samples=100, n_features=1, noise=0.1, random_state=42)\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "325a7367-5c80-41a8-96bd-2a3b58aafcf1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train the gradient boosting model\n",
    "gb = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "gb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3796ea7e-fe0e-4d1a-af26-9ee736736b26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred = gb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "207d69d1-6170-4f19-ab79-e74feacf113b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 1.3379888778506104\n",
      "R-squared: 0.9990403356176427\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R-squared: {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440b4136-e453-4293-b89e-3b45211f44a2",
   "metadata": {},
   "source": [
    "# Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to optimise the performance of the model. Use grid search or random search to find the best hyperparameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e96522a-5239-4d71-b709-b0cb371c1b2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "class GradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.trees = []\n",
    "        self.tree_weights = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Initialize the model predictions as the mean of the target variable\n",
    "        self.base_prediction = np.mean(y)\n",
    "        y_pred = np.full_like(y, self.base_prediction)\n",
    "        \n",
    "        for _ in range(self.n_estimators):\n",
    "            # Calculate the residuals\n",
    "            residuals = y - y_pred\n",
    "\n",
    "            # Fit a regression tree to the residuals\n",
    "            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "            tree.fit(X, residuals)\n",
    "\n",
    "            # Calculate the tree weight using the learning rate\n",
    "            tree_weight = self.learning_rate\n",
    "            self.tree_weights.append(tree_weight)\n",
    "\n",
    "            # Update the model predictions\n",
    "            y_pred += tree_weight * tree.predict(X)\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Initialize predictions as the base prediction\n",
    "        y_pred = np.full(X.shape[0], self.base_prediction)\n",
    "\n",
    "        # Add the predictions of each tree weighted by the tree weight\n",
    "        for tree, tree_weight in zip(self.trees, self.tree_weights):\n",
    "            y_pred += tree_weight * tree.predict(X)\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return {\n",
    "            'n_estimators': self.n_estimators,\n",
    "            'learning_rate': self.learning_rate,\n",
    "            'max_depth': self.max_depth\n",
    "        }\n",
    "\n",
    "    def set_params(self, **params):\n",
    "        for param, value in params.items():\n",
    "            setattr(self, param, value)\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bfa6f9a-0072-403f-9a8e-9fab72ed7e98",
   "metadata": {},
   "source": [
    "# Q4. What is a weak learner in Gradient Boosting?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a925bac8-5bce-456f-94c1-2ba97da93f91",
   "metadata": {},
   "source": [
    "In Gradient Boosting, a weak learner is a simple model that performs slightly better than random guessing on a classification or regression problem. In the context of Gradient Boosting, weak learners are typically decision trees with a shallow depth (e.g., a maximum depth of 1 or 2 for classification problems, and a small maximum depth for regression problems).\n",
    "\n",
    "The key characteristic of a weak learner is that it is only required to perform slightly better than random chance, as the boosting process will combine multiple weak learners to create a strong learner. Each weak learner focuses on a specific subset of the data or a specific aspect of the problem, and by combining the predictions of all the weak learners, the Gradient Boosting algorithm is able to improve the overall performance and generalize well to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de7184d-afb3-4001-a25d-190a5bc56353",
   "metadata": {},
   "source": [
    "# Q5. What is the intuition behind the Gradient Boosting algorithm?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24265b35-8161-40f5-bdf0-480414747518",
   "metadata": {},
   "source": [
    "The intuition behind the Gradient Boosting algorithm can be summarized as follows:\n",
    "\n",
    "1. **Sequential Learning:** Gradient Boosting builds an ensemble model (often decision trees) sequentially. Each new model in the sequence corrects the errors made by the previous models. This sequential learning process allows the model to focus on the hard-to-predict examples.\n",
    "\n",
    "2. **Gradient Descent:** The \"Gradient\" in Gradient Boosting refers to the technique of using gradients (derivatives) of the loss function to minimize the loss. In each iteration, the algorithm calculates the gradient of the loss function with respect to the current model's predictions and fits a new model to the residuals (the differences between the actual and predicted values).\n",
    "\n",
    "3. **Gradient Descent in Function Space:** Unlike traditional gradient descent, which updates the parameters of a model, Gradient Boosting updates the function space. Each new model (weak learner) is added to the ensemble to reduce the error of the overall model in the function space.\n",
    "\n",
    "4. **Regularization:** Gradient Boosting includes a regularization parameter to control the complexity of the ensemble. This helps prevent overfitting by penalizing overly complex models.\n",
    "\n",
    "5. **Combining Weak Learners:** By combining multiple weak learners (simple models) into a strong learner (complex model), Gradient Boosting is able to create a highly flexible and powerful model that can capture complex patterns in the data.\n",
    "\n",
    "the intuition behind Gradient Boosting is to iteratively improve the model's predictions by focusing on the errors made by the previous models and combining multiple weak learners to create a strong predictive model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd9546b-2d1c-4d56-b94f-17b029e11f23",
   "metadata": {},
   "source": [
    "# Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed90d4f-be11-49d4-b62b-8188a014d453",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e8f0f2fe-fc00-4091-9013-c9d03218cb92",
   "metadata": {},
   "source": [
    "# Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c13391-10d5-4039-9b22-29302d003170",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
